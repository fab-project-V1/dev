# âš™ï¸ Model Quantization in Fabric

Quantization reduces the numerical precision of model weights and activations, enabling faster, smaller, and more energy-efficient inferenceâ€”critical for edge and real-time AI.

---

## ğŸ¯ What Is Quantization?

Quantization converts model parameters from 32-bit floating point (FP32) to lower-bit representations like:

| Format   | Bit Width | Use Case                  |
|----------|-----------|---------------------------|
| FP16     | 16-bit    | Speed boost, same accuracy |
| INT8     | 8-bit     | Edge deployment, low power |
| INT4     | 4-bit     | Max compression, some tradeoff |

---

## âš¡ Benefits

- ğŸš€ **Speedup:** Reduced computational demand
- ğŸ“¦ **Size reduction:** Smaller model binaries
- ğŸ”‹ **Energy savings:** Especially on mobile/IoT
- ğŸŒ **Latency improvement:** Faster responses

---

## ğŸ§° Quantizing in Fabric

Fabric supports quantization via the `fab optimize` tool:

```bash
fab optimize quantize --model models/agent-repacker/1.0.0 --precision int8
Outputs a quantized/ folder alongside original weights.

ğŸ›  Supported Formats
Framework	Quantization APIs
ONNX	ONNX Runtime QDQ/ORT tools
PyTorch	torch.quantization or optimum
TensorFlow	Post-training quantization via TFLite

Fabric's optimizer selects the correct backend automatically.

ğŸ“‰ Accuracy vs Compression Tradeoff
Format	Compression	Accuracy Impact
FP16	~2x	Negligible
INT8	~4x	Minor (<1%)
INT4	~8x	Moderate (2â€“5%)

Evaluate quantized models using:

bash

fab benchmark agent agent-repacker --quantized
ğŸ”’ Compliance Notes
Quantized models must retain original provenance

Include a new parameter_count in your model.yaml

You may sign separately with:

bash

fab model sign --variant quantized
ğŸ§ª Hybrid and Layer-wise Quantization
Advanced users can:

Mix FP16 and INT8 per layer

Preserve attention or embedding layers in FP32

Use per-channel vs per-tensor quantization

Fabric does not yet support mixed-precision auto-tuning, but it's planned for v1.2.

Quantization is a powerful knobâ€”use it wisely to balance performance and model fidelity.
